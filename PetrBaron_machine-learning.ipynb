{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install hyperopt\n!ls -a /kaggle/input/jane-street-market-prediction/train.csv\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n#from autoviz.AutoViz_Class import AutoViz_Class\n#AV = AutoViz_Class()\n#dft = AV.AutoViz(\"\", sep=',', depVar='resp', dfte=df, header=0, verbose=2,\n#                            lowess=False,chart_format='svg',max_rows_analyzed=1500,max_cols_analyzed=30)\nimport matplotlib.pyplot as plt\n\ndef get_most_possible_profit(stock_prices, weight):\n    most_possible_profit = 0.0\n    for i in range(1,len(stock_prices)):\n        if (stock_prices[i-1] < stock_prices[i]):\n            most_possible_profit += (stock_prices[i] - stock_prices[i-1])*weight[i]\n    return most_possible_profit\n\ndef evaluate(datafr):\n    p_i = []\n    number_of_days = len(pd.unique( datafr.date.to_list() ) )\n    for i in range( number_of_days  ):\n        current_day_df = datafr[datafr.date == i]\n        p_i_current_day = np.array( current_day_df.weight * current_day_df.resp * current_day_df.action ).sum()\n        p_i.append(p_i_current_day)\n    \n    np_p_i = np.array(p_i)\n    t = ( np_p_i.sum()/ np.sqrt( (np_p_i * np_p_i).sum() ) ) * np.sqrt(250/number_of_days)\n    utility = min(max(0,t), 6 ) * np_p_i.sum()\n    return utility\n            \n\n\ndf = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv', nrows = 5000)\n#df = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\ndf = df[df.weight>0.0]\ndf.reset_index(inplace = True, drop = True) \n#df['action'] = np.random.randint(2, size=len(df.index))\ndf['action'] = np.zeros(len(df.index))\n\nprint(df.info )\nprint(df.head(10))\n\nutil = evaluate(df) \nprint(\"Util = \", util)\n\n            \nfrom sklearn.datasets import make_gaussian_quantiles\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\ndf.loc[df['resp'] > 0.0, 'action'] = 1\ndf.loc[df['resp'] <= 0.0, 'action'] = 0\nprofit_columns = []\nloose_columns = []\ndf = df.dropna()\nfor (columnName, columnData) in df.iteritems(): \n    if \"date\" in columnName:\n        continue \n    if \"resp\" in columnName:\n        continue \n    if \"action\" in columnName:\n        continue \n    profit_columns.append(df[columnName][df.action > 0.0].to_list())\n    loose_columns.append(df[columnName][df.action <= 0.0].to_list())\n\nX1 = np.array ( profit_columns  ) \ny1 = np.array ( df.action[df.action>0.0].to_list()  ) \nX2 = np.array ( loose_columns  ) \ny2 = np.array ( df.action[df.action<=0.0].to_list()  ) \n\nX1 = X1.transpose()\nX2 = X2.transpose()\nprint(\"x: \" , X1)\nprint(\"x shape: \" , X1.shape)\nprint(\"type: \" , type(X1))\nprint(\"y: \" , y1)\nprint(\"y shape: \" , y1.shape)\nprint(\"type: \" , type(y1))\n\n\nX = np.concatenate((X1, X2))\ny = np.concatenate((y1, y2))\n\n# Split data into train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\nspace = {\n\n            #'units1': hp.quniform('units1', 32,3*1024,1),\n            #'dropout1': hp.uniform('dropout1', .00,.50),\n            'maxdepth1': hp.uniform('maxdepth1', 1,20),\n            'maxdepth2': hp.uniform('maxdepth2', 1,20),\n            'maxdepth3': hp.uniform('maxdepth3', 1,20),\n            'maxdepth4': hp.uniform('maxdepth4', 1,20),\n            #'est1': hp.uniform('est1', 1.0,10000),\n            #'est2': hp.uniform('est2', 1.0,10000),\n            #'est3': hp.uniform('est3', 1.0,10000),\n            'method': hp.uniform('method', 0.0,4.0),\n        \n\n        }\ndef f_nn(params):   \n    if int(params['method']) == 0:\n        clf = DecisionTreeClassifier(max_depth=int(params['maxdepth1']))\n        #clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=int(params['maxdepth2'])),n_estimators=int(params['est1']))\n        #clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=int(params['maxdepth3'])),n_estimators=int(params['est2']))\n        #clf = BaggingClassifier(DecisionTreeClassifier(max_depth=int(params['maxdepth4'])),n_estimators=int(params['est3']))\n        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=int(params['maxdepth2'])),n_estimators=6000)\n        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=int(params['maxdepth3'])),n_estimators=600)\n        clf = BaggingClassifier(DecisionTreeClassifier(max_depth=int(params['maxdepth4'])),n_estimators=100)\n    if int(params['method']) == 1:\n        clf = GaussianNB()\n        clf = AdaBoostClassifier(learning_rate=2.5,base_estimator=GaussianNB(),n_estimators=800,algorithm='SAMME.R') #,algorithm='SAMME.R')\n        clf = BaggingClassifier(GaussianNB(),max_samples=0.009,n_estimators=500)\n    if int(params['method']) == 2:\n        clf = KNeighborsClassifier(n_neighbors=1)\n        clf = BaggingClassifier(KNeighborsClassifier(n_neighbors=1),n_estimators=500)\n    #if int(params['method']) == 3:\n    #    clf = LinearDiscriminantAnalysis()\n    #    clf = BaggingClassifier(LinearDiscriminantAnalysis(),max_samples=0.009,n_estimators=10000)\n    if int(params['method']) == 3:\n        clf = GradientBoostingClassifier(learning_rate=0.1,n_estimators=200)\n        clf = RandomForestClassifier(n_estimators=200)\n    \n       \n    clf.fit(X_train, y_train)\n\n    aa = np.mgrid[-100:100:10000]\n    Z = clf.predict(np.c_[ aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel() ])                        \n    ZZ = clf.predict_proba(X_test)[:, 1]\n    fpr, tpr, _ = roc_curve(y_test, ZZ)\n    auc = roc_auc_score(y_test, ZZ)\n    print(\"AUC: \", auc)\n\n    return {'loss': -auc, 'status': STATUS_OK}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trials = Trials()\nbest = fmin(fn=f_nn, space=space, algo=tpe.suggest, max_evals=10, trials=trials)\nprint('best: ', best)\nprint('best accuracy: ',-trials.best_trial['result']['loss'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and fit a classifier\nif int(best['method']) == 0:\n    clf = DecisionTreeClassifier(max_depth=int(best['maxdepth1']))\n    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=int(best['maxdepth2'])),n_estimators=6000)\n    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=int(best['maxdepth3'])),n_estimators=600)\n    clf = BaggingClassifier(DecisionTreeClassifier(max_depth=int(best['maxdepth4'])),n_estimators=100)\nif int(best['method']) == 1:\n    clf = GaussianNB()\n    clf = AdaBoostClassifier(learning_rate=2.5,base_estimator=GaussianNB(),n_estimators=800,algorithm='SAMME.R') #,algorithm='SAMME.R')\n    clf = BaggingClassifier(GaussianNB(),max_samples=0.009,n_estimators=500)\nif int(best['method']) == 2:\n    clf = KNeighborsClassifier(n_neighbors=1)\n    clf = BaggingClassifier(KNeighborsClassifier(n_neighbors=1),n_estimators=10000)\n#if int(best['method']) == 3:\n#    clf = LinearDiscriminantAnalysis()\n#    clf = BaggingClassifier(LinearDiscriminantAnalysis(),max_samples=0.009,n_estimators=int(best['est1']))\nif int(best['method']) == 3:\n    clf = GradientBoostingClassifier(learning_rate=0.1,n_estimators=200)\n    clf = RandomForestClassifier(n_estimators=200)\n\n# Takes extremaly long time\n#clf = GaussianProcessClassifier(1.0 * RBF(1.0))\n#clf = BaggingClassifier(GaussianProcessClassifier(1.0 * RBF(1.0)))\n\nclf.fit(X_train, y_train)\n\nplot_colors = \"br\"\nplot_step = 0.1\nclass_names = \"AB\"\n \nplt.figure(figsize=(10, 5))\nx_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\ny_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1 \nxx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                     np.arange(y_min, y_max, plot_step))\naa = np.mgrid[-100:100:10000]\nZ = clf.predict(np.c_[ aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel(), aa.ravel() ])\n#Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nprint(\"Z \", Z)\nprint(\"Z  shape\", Z.shape)\n#Z = Z.reshape(xx.shape)\nprint(\"Z \", Z)\nprint(\"Z re-shape\", Z.shape)\n#cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n#splt.axis(\"tight\")\n \n# Plot the test points","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(10,10))\nplt.ylabel(\"Returns\")\nplt.xlabel(\"Time\")\nplt.title('Stock price returns')\nplt.plot(df.resp[0:100])\nplt.savefig('/kaggle/working/returns.pdf')\nplt.savefig('/kaggle/working/returns.png')\n\nplt.figure(2,figsize=(10,10))\nplt.ylabel(\"Events\")\nplt.xlabel(\"Returns\")\nplt.title('Histogram of returns')\nplt.hist(df.resp[0:1000], np.linspace(-0.5,0.5,100))\nplt.savefig('/kaggle/working/returns_hist.pdf')\nplt.savefig('/kaggle/working/returns_hist.png')\n\nplt.figure(3,figsize=(10,10))\nplt.ylabel(\"Feature_15\")\nplt.xlabel(\"Time\")\nplt.title('Stock Feature_15')\nplt.plot(df.feature_15[0:50])\nplt.savefig('/kaggle/working/feature_15.pdf')\nplt.savefig('/kaggle/working/feature_15.png')\n\nplt.figure(4,figsize=(10,10))\nplt.ylabel(\"Feature_15\")\nplt.xlabel(\"Feature_14\")\nplt.title('Feature_14 vs Feature_15')\nplt.scatter(df.feature_14[0:1000],df.feature_15[0:1000])\nplt.savefig('/kaggle/working/feature_14_vs_15.pdf')\nplt.savefig('/kaggle/working/feature_14_vs_15.png')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Plotting:\")\nfigure_i_min = 100\nfigure_j_min = 100\nfigure_i = 120\nfigure_j = 120\nfig, axs = plt.subplots(figure_i-figure_i_min, figure_j-figure_j_min, figsize=(20,20))\nfor i in range(figure_i-figure_i_min):\n    for j in range (figure_j - figure_j_min): \n        for k, n, c in zip(range(2), class_names, plot_colors):\n            # plot first 400 test data points\n            N = min(400,len(y_test))\n            #N = len(y_test)\n            idx = np.where(y_test[0:N] == k)\n            #print(\"Plotting \",len(idx),\" datapoints., i, j : \", i,j)\n            axs[i,j].scatter(X_test[idx, figure_i_min + i], X_test[idx, figure_j_min + j],\n                             c=c, cmap=plt.cm.Paired,\n                             label= str(figure_i_min + i) + \" vs. \" + str(figure_j_min + j), alpha=1.0)\n            axs[i,j].xaxis.set_ticks([]) \n            axs[i,j].yaxis.set_ticks([]) \n            axs[i,j].xaxis.set_ticklabels([])\n            axs[i,j].yaxis.set_ticklabels([])\n            #axs[i,j].legend()\nfig.tight_layout()\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.savefig(\"/kaggle/working/Features_zoomed.pdf\")\nplt.savefig(\"/kaggle/working/Features_zoomed.png\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def GetCorrDf(k = 1, l = 1, Profit=True):\n    if Profit:\n        return np.corrcoef(df['feature_'+str(k)][df.action > 0.0], df['feature_'+str(l)][df.action > 0.0])[0][1]\n    else:\n        return np.corrcoef(df['feature_'+str(k)][df.action <= 0.0], df['feature_'+str(l)][df.action <= 0.0])[0][1]\n    \n\n#PlotFeatures(4,48)\n\nprofit_corr = []\nloose_corr = []\nfor i in range(130):\n    profit_row = []\n    loose_row = []\n    for j in range(130):\n        profit_row.append(GetCorrDf(i,j))\n        loose_row.append(GetCorrDf(i,j, False))\n    loose_corr.append(loose_row)    \n    profit_corr.append(profit_row)\n\nprint(\"Done.\")\nimport seaborn as sns; sns.set()\nfig, axs = plt.subplots(1, 2, figsize=(25,10))\naxs[0] = sns.heatmap(profit_corr,ax=axs[0], vmin=-1.0, vmax=1.0)\naxs[0].invert_yaxis()\naxs[0].set_title(\"Profit Heatmap Original Data\")\naxs[0].set_xlabel(\"Feature number\")\naxs[1] = sns.heatmap(loose_corr, ax=axs[1], vmin=-1.0, vmax=1.0)\naxs[1].invert_yaxis()\naxs[1].set_title(\"Loose Heatmap Original Data\")\naxs[1].set_xlabel(\"Feature number\")\nplt.savefig(\"/kaggle/working/Heatmap_df.pdf\")\nplt.savefig(\"/kaggle/working/Heatmap_df.png\")\nplt.show()\n\ndef GetCorr(k = 1, l = 1):\n    corrcoefs = []\n    for m, n, c in zip(range(2), class_names, plot_colors):\n            N = len(y_test)\n            idx = np.where(y_test[0:N] == m)\n            corrcoefs.append(np.corrcoef(X_test[idx, k], X_test[idx, l])[0][1])\n    return corrcoefs\n\n#PlotFeatures(4,48)\n\nprofit_corr = []\nloose_corr = []\nfor i in range(130):\n    profit_row = []\n    loose_row = []\n    for j in range(130):\n        profit_row.append(GetCorr(i,j)[0])\n        loose_row.append(GetCorr(i,j)[1])\n    loose_corr.append(loose_row)    \n    profit_corr.append(profit_row)\n\nprint(\"Done.\")\nimport seaborn as sns; sns.set()\nfig, axs = plt.subplots(1, 2, figsize=(25,10))\naxs[0] = sns.heatmap(profit_corr,ax=axs[0], vmin=-1.0, vmax=1.0)\naxs[0].invert_yaxis()\naxs[0].set_title(\"Profit Heatmap After Classification\")\naxs[0].set_xlabel(\"Feature number\")\naxs[1] = sns.heatmap(loose_corr, ax=axs[1], vmin=-1.0, vmax=1.0)\naxs[1].invert_yaxis()\naxs[1].set_title(\"Loose Heatmap After Classification\")\naxs[1].set_xlabel(\"Feature number\")\nplt.savefig(\"/kaggle/working/Heatmap_classes.pdf\")\nplt.savefig(\"/kaggle/working/Heatmap_classes.png\")\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# plot the roc curve\nfigure = plt.figure(figsize=(10, 5))\nZZ = clf.predict_proba(X_test)[:, 1]\nprint(X_test)\nfpr, tpr, _ = roc_curve(y_test, ZZ)\nplt.plot(fpr, tpr, label=\"ROC\")\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\n#plt.title('ROC curve')\nplt.legend(loc='best')\nauc = roc_auc_score(y_test, ZZ)\nplt.title(\"Test sample\")\nplt.text(0.7, 0.5, ('AUC = %.2f' % auc),\n        size=15, horizontalalignment='right')\nplt.savefig(\"/kaggle/working/roc.pdf\")\nplt.savefig(\"/kaggle/working/roc.png\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def PlotFeatures(k = 1, l = 1):\n    plt.figure(int(str(k)+str(l)) ,figsize = (10,10))\n    ax = plt.subplot(111)\n    corrcoefs = []\n    for m, n, c in zip(range(2), class_names, plot_colors):\n            # plot first 400 test data points\n            N = min(400,len(y_test))\n            #N = len(y_test)\n            idx = np.where(y_test[0:N] == m)\n            corrcoefs.append(np.corrcoef(X_test[idx, k], X_test[idx, l])[0][1])\n            ax.scatter(X_test[idx, k], X_test[idx, l],\n                             c=c, cmap=plt.cm.Paired,\n                             label=\"Feature\"+str(k) +\" vs. \"+ str(l), alpha=1.0) \n            ax.set_xlabel('Feature '+str(k))\n            ax.set_ylabel('Feature '+str(l))\n    plt.savefig(os.path.join(os.getcwd(), 'feature_'+str(k)+'_vs_'+str(l)+'.pdf'))\n    plt.show()\n    return corrcoefs\nPlotFeatures(41,48)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}